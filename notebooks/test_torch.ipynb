{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.utils import HadamardTransform\n",
    "from utils.hadamard_utils import matmul_hadU_cuda, get_hadK\n",
    "import math\n",
    "import numpy as np\n",
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the bit pattern of a floating-point number\n",
    "def float_to_hex(f):\n",
    "    # Pack the float into 4 bytes (32-bit float) or 8 bytes (64-bit float)\n",
    "    packed = struct.pack('>f', f)  # Use '>f' for 32-bit float, '>d' for 64-bit double\n",
    "    # Unpack as an integer to get the bit pattern\n",
    "    hex_value = ''.join(f'{byte:02x}' for byte in packed)\n",
    "    return hex_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.rand(2,2,11008, dtype=torch.float32, device='cuda')\n",
    "q_pad = torch.concat([q, torch.zeros(q.shape[0], q.shape[1], 5376, dtype=q.dtype, device=\"cuda\")], dim=-1)\n",
    "\n",
    "# q = torch.rand(2,2,11008, dtype=torch.float32)\n",
    "# q_pad = torch.concat([q, torch.zeros(q.shape[0], q.shape[1], 5376, dtype=q.dtype)], dim=-1)\n",
    "\n",
    "np.log2([11008])\n",
    "print(2**14)\n",
    "print(16384-11008)\n",
    "\n",
    "dtype = q.dtype\n",
    "q_ref = (HadamardTransform.apply(q.float()) / math.sqrt(q.shape[-1])).to(dtype)\n",
    "q_evl = (HadamardTransform.apply(q_pad.float()) / math.sqrt(q.shape[-1])).to(dtype)\n",
    "\n",
    "print((q_ref-q_evl[...,:11008]).abs().mean())\n",
    "print(((q_ref-q_evl[...,:11008]).abs()/(q_ref.abs() + 1e-6)).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n1, n2, n3 = 2, 2, 4*172\n",
    "q = torch.zeros(n1*n2*n3, dtype=torch.float32)\n",
    "for i in range(n1*n2*n3):\n",
    "  q[i] = i/(n1*n2*n3)\n",
    "q = q.reshape(n1, n2, n3)\n",
    "\n",
    "\n",
    "had_K, K = get_hadK(q.shape[-1])\n",
    "q_had = matmul_hadU_cuda(q, had_K, K)\n",
    "\n",
    "print(had_K)\n",
    "print(K)\n",
    "print(q_had)\n",
    "\n",
    "data = q_had.numpy()\n",
    "data.tofile('./hadamard_test/hadamard_transform.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_had_ref = q_had.reshape(-1, n3).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(q_had_ref.shape[0]):\n",
    "    for j in range(q_had_ref.shape[1]):\n",
    "        hex_value = float_to_hex(q_had_ref[i, j].item())\n",
    "        print(f\"0x{hex_value}\", end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1100.0, -1110.1, 7.6, 8.4], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.to(torch.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spinquant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
