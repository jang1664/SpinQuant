{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# SPINQUANT_HOME = os.environ.get(\"SPINQUANT_HOME\", \"/opt/spinquant\")\n",
    "SPINQUANT_HOME = '/home/jaeyongjang/project.local/SpinQuant'\n",
    "\n",
    "os.chdir(SPINQUANT_HOME)\n",
    "print(\"Changed working directory to:\", os.getcwd())\n",
    "\n",
    "sys.argv = [\n",
    "  \"python\",\n",
    "  \"--input_model\", \"models/llama2-7b\",\n",
    "  \"--do_train\", \"False\",\n",
    "  \"--do_eval\", \"True\",\n",
    "  \"--per_device_eval_batch_size\", \"4\",\n",
    "  \"--model_max_length\", \"2048\",\n",
    "  \"--fp16\", \"True\",\n",
    "  \"--bf16\", \"False\",\n",
    "  \"--save_safetensors\", \"False\",\n",
    "  \"--w_bits\", \"4\",\n",
    "  \"--a_bits\", \"16\",\n",
    "  \"--k_bits\", \"4\",\n",
    "  \"--v_bits\", \"4\",\n",
    "  \"--w_clip\",\n",
    "  \"--a_asym\",\n",
    "  \"--k_asym\",\n",
    "  \"--v_asym\",\n",
    "  \"--rotate\",\n",
    "  \"--k_groupsize\", \"128\",\n",
    "  \"--v_groupsize\", \"128\",\n",
    "  \"--load_qmodel_path\", \"saved_models/llama2-7b/a16w4kv4-vasym.pt\",\n",
    "  \"--optimized_rotation_path\", \"rotation_llama-2-7b/a16w4kv4-vsym/R.bin\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from logging import Logger\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from transformers import LlamaTokenizerFast\n",
    "import transformers\n",
    "from eval_utils.main import ptq_model\n",
    "from eval_utils.modeling_llama import LlamaForCausalLM\n",
    "from utils import data_utils, eval_utils, utils\n",
    "from utils.process_args import process_args_ptq\n",
    "\n",
    "log: Logger = utils.get_logger(\"spinquant\")\n",
    "\n",
    "import evaluate\n",
    "from lm_eval import evaluator\n",
    "from lm_eval.utils import make_table\n",
    "\n",
    "from utils.quant_utils import find_qlayers, ActQuantWrapper\n",
    "from functools import partial\n",
    "import pickle\n",
    "\n",
    "from utils.profile import (\n",
    "  measure, profile, get_profiler, \n",
    "  get_profiled_df, plot_profiled_df,\n",
    "  run_profile\n",
    ")\n",
    "import pstats\n",
    "import importlib\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_names = ['hellaswag', 'arc_easy','arc_challenge', 'winogrande', 'openbookqa', \"wikitext\"]\n",
    "# task_names = ['openbookqa']\n",
    "# task_names = ['arc_easy']\n",
    "\n",
    "CUDA_DEVICES = list(map(str.strip, os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"0\").split(\",\")))\n",
    "FIRST_GPU_ID = int(CUDA_DEVICES[0])\n",
    "GPU_ID = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist.init_process_group(backend=\"nccl\", timeout=datetime.timedelta(hours=8))\n",
    "model_args, training_args, ptq_args = process_args_ptq()\n",
    "print(\"------- ARGS ----------\")\n",
    "print(\"-----model args-----\")\n",
    "print(model_args)\n",
    "print(\"------train args-------\")\n",
    "print(training_args)\n",
    "print(\"-------- ptq args ---------\")\n",
    "print(ptq_args)\n",
    "print(\"------- ARGS END ----------\")\n",
    "\n",
    "config = transformers.AutoConfig.from_pretrained(\n",
    "    model_args.input_model, token=model_args.access_token, attn_implementation=\"eager\"\n",
    ")\n",
    "# Llama v3.2 specific: Spinquant is not compatiable with tie_word_embeddings, clone lm_head from embed_tokens\n",
    "process_word_embeddings = False\n",
    "if config.tie_word_embeddings:\n",
    "    config.tie_word_embeddings = False\n",
    "    process_word_embeddings = True\n",
    "dtype = torch.bfloat16 if training_args.bf16 else torch.float16\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_args.input_model,\n",
    "    config=config,\n",
    "    torch_dtype=dtype,\n",
    "    token=model_args.access_token,\n",
    ")\n",
    "if process_word_embeddings:\n",
    "    model.lm_head.weight.data = model.model.embed_tokens.weight.data.clone()\n",
    "model.cuda()\n",
    "\n",
    "model = ptq_model(ptq_args, model, model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.seqlen = training_args.model_max_length\n",
    "log.info(\"Model PTQ completed {}\".format(model))\n",
    "log.info(\"Start to load tokenizer...\")\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_args.input_model,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    model_max_length=training_args.model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=True,\n",
    "    add_eos_token=False,\n",
    "    add_bos_token=False,\n",
    "    token=model_args.access_token,\n",
    ")\n",
    "log.info(\"Complete tokenizer loading...\")\n",
    "# model.config.use_cache = False\n",
    "# dataset_ppl = eval_utils.evaluator(model, testloader, utils.DEV, ptq_args)\n",
    "# log.info(\"wiki2 ppl is: {}\".format(dataset_ppl))\n",
    "# dist.barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# model.cpu()\n",
    "# del input_ids, past_key_values\n",
    "# gc.collect()\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_seq_len = 0\n",
    "target_seq_len = 32\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./prof\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = [1, 2, 4]\n",
    "past_seq_len = [8, 16, 32, 64, 128]\n",
    "seq_len = [8, 16, 32, 64, 128]\n",
    "\n",
    "for bs in batch_size:\n",
    "  # prefill\n",
    "  for sl in seq_len:\n",
    "    print(f\"PREFILL: bs {bs}, sl {sl}\")\n",
    "    run_profile(model, bs, 0, sl, \"cpu\", f\"./prof/bs{bs}_sl{sl}_prf.png\") \n",
    "  \n",
    "  # generate\n",
    "  for sl in past_seq_len:\n",
    "    print(f\"GENERATE: bs {bs}, sl {sl}\")\n",
    "    run_profile(model, bs, sl, 1, \"cpu\", f\"./prof/bs{bs}_sl{sl}_gen.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = [1, 2, 4]\n",
    "past_seq_len = [8, 16, 32, 64, 128]\n",
    "seq_len = [8, 16, 32, 64, 128]\n",
    "\n",
    "for bs in batch_size:\n",
    "  # prefill\n",
    "  for sl in seq_len:\n",
    "    print(f\"PREFILL: bs {bs}, sl {sl}\")\n",
    "    run_profile(model, bs, 0, sl, \"cuda\", f\"./prof/bs{bs}_sl{sl}_prf-cuda.png\") \n",
    "  \n",
    "  # generate\n",
    "  for sl in past_seq_len:\n",
    "    print(f\"GENERATE: bs {bs}, sl {sl}\")\n",
    "    run_profile(model, bs, sl, 1, \"cuda\", f\"./prof/bs{bs}_sl{sl}_gen-cuda.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
