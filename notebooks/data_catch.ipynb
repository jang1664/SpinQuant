{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import datetime\n",
    "from logging import Logger\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from transformers import LlamaTokenizerFast\n",
    "import transformers\n",
    "from eval_utils.main import ptq_model\n",
    "from eval_utils.modeling_llama import LlamaForCausalLM\n",
    "from utils import data_utils, eval_utils, utils\n",
    "from utils.process_args import process_args_ptq\n",
    "\n",
    "import evaluate\n",
    "from lm_eval import evaluator\n",
    "from lm_eval.utils import make_table\n",
    "\n",
    "from utils.quant_utils import find_qlayers, ActQuantWrapper\n",
    "from functools import partial\n",
    "import pickle\n",
    "\n",
    "from utils.profile import (\n",
    "  measure, profile, get_profiler, \n",
    "  get_profiled_df, plot_profiled_df,\n",
    "  run_profile\n",
    ")\n",
    "import pstats\n",
    "import importlib\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import functools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "log: Logger = utils.get_logger(\"spinquant\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "sys.argv = [\n",
    "  \"python\",\n",
    "  \"--input_model\", \"../models/llama2-7b\",\n",
    "  \"--do_train\", \"False\",\n",
    "  \"--do_eval\", \"True\",\n",
    "  \"--per_device_eval_batch_size\", \"4\",\n",
    "  \"--model_max_length\", \"2048\",\n",
    "  \"--fp16\", \"True\",\n",
    "  \"--bf16\", \"False\",\n",
    "  \"--save_safetensors\", \"False\",\n",
    "  \"--w_bits\", \"4\",\n",
    "  \"--a_bits\", \"16\",\n",
    "  \"--k_bits\", \"4\",\n",
    "  \"--v_bits\", \"4\",\n",
    "  \"--w_clip\",\n",
    "  \"--a_asym\",\n",
    "  \"--k_asym\",\n",
    "  \"--v_asym\",\n",
    "  \"--rotate\",\n",
    "  \"--k_groupsize\", \"128\",\n",
    "  \"--v_groupsize\", \"128\",\n",
    "  \"--load_qmodel_path\", \"../saved_models/llama2-7b/a16w4kv4-vasym.pt\",\n",
    "  \"--optimized_rotation_path\", \"../rotation_llama-2-7b/a16w4kv4-vasym/R.bin\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist.init_process_group(backend=\"nccl\", timeout=datetime.timedelta(hours=8))\n",
    "model_args, training_args, ptq_args = process_args_ptq()\n",
    "print(\"------- ARGS ----------\")\n",
    "print(\"-----model args-----\")\n",
    "print(model_args)\n",
    "print(\"------train args-------\")\n",
    "print(training_args)\n",
    "print(\"-------- ptq args ---------\")\n",
    "print(ptq_args)\n",
    "print(\"------- ARGS END ----------\")\n",
    "\n",
    "config = transformers.AutoConfig.from_pretrained(\n",
    "    model_args.input_model, token=model_args.access_token, attn_implementation=\"eager\"\n",
    ")\n",
    "# Llama v3.2 specific: Spinquant is not compatiable with tie_word_embeddings, clone lm_head from embed_tokens\n",
    "process_word_embeddings = False\n",
    "if config.tie_word_embeddings:\n",
    "    config.tie_word_embeddings = False\n",
    "    process_word_embeddings = True\n",
    "dtype = torch.bfloat16 if training_args.bf16 else torch.float16\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_args.input_model,\n",
    "    config=config,\n",
    "    torch_dtype=dtype,\n",
    "    token=model_args.access_token,\n",
    ")\n",
    "if process_word_embeddings:\n",
    "    model.lm_head.weight.data = model.model.embed_tokens.weight.data.clone()\n",
    "model.cuda()\n",
    "\n",
    "model = ptq_model(ptq_args, model, model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.seqlen = training_args.model_max_length\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_args.input_model,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    model_max_length=training_args.model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=True,\n",
    "    add_eos_token=False,\n",
    "    add_bos_token=False,\n",
    "    token=model_args.access_token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_dict(d, key, value, max_length=None):\n",
    "  if key not in d:\n",
    "    d[key] = []\n",
    "  if max_length is not None:\n",
    "    if len(d[key]) >= max_length:\n",
    "      d[key].pop(random.choice(range(len(d[key]))))\n",
    "  d[key].append(value)\n",
    "\n",
    "\n",
    "def is_linear(module):\n",
    "  return isinstance(module, torch.nn.Linear) and (hasattr(module, 'weight'))\n",
    "\n",
    "def hook_to_linear(model, inputs, outputs, max_length=1024):\n",
    "  def forward_hook(module, input, output, name=None):\n",
    "    append_to_dict(inputs, name, input[0].detach().cpu(), max_length=max_length)\n",
    "    append_to_dict(outputs, name, output.detach().cpu(), max_length=max_length)\n",
    "\n",
    "  handles = []\n",
    "  for name, module in model.named_modules():\n",
    "    if is_linear(module):\n",
    "      handles.append(module.register_forward_hook(functools.partial(forward_hook, name=name)))\n",
    "  \n",
    "  return handles\n",
    "\n",
    "def extract_weights(model):\n",
    "  weights = {}\n",
    "  for name, module in model.named_modules():\n",
    "    if is_linear(module) and \"lm_head\" not in name:\n",
    "      weights[name] = module.weight.detach().cpu()\n",
    "  return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, outputs, weights = {}, {}, {}\n",
    "handles = hook_to_linear(model, inputs, outputs, max_length=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = extract_weights(model)\n",
    "weights = {k.replace(\".module\",\"\"): v for k, v in weights.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_names = ['hellaswag', 'arc_easy','arc_challenge', 'winogrande', 'openbookqa']\n",
    "task_names = ['hellaswag']\n",
    "\n",
    "for task_name in task_names:\n",
    "  inputs.clear()\n",
    "  outputs.clear()\n",
    "\n",
    "  results = evaluator.simple_evaluate(\n",
    "      model=\"hf\",\n",
    "      model_args={\"pretrained\" : model.cuda(),\n",
    "                  \"tokenizer\" : tokenizer},\n",
    "      tasks=[task_name],\n",
    "      num_fewshot=0,\n",
    "      batch_size=1,\n",
    "      limit=1,\n",
    "      device=\"cuda\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(inputs, open(f\"inputs_{task_name}.pkl\", \"wb\"))\n",
    "pickle.dump(outputs, open(f\"outputs_{task_name}.pkl\", \"wb\"))\n",
    "pickle.dump(weights, open(f\"weights_{task_name}.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spinquant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
